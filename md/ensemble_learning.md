# ensemble learning(集成学习)
个体学习器选择
（1）个体学习器时同质的，比如决策树/神经网络
（2）个体学习器时异质的，比如支持向量机，逻辑回归，朴素贝叶斯
（3）通知个体学习器应用最广泛，其中CART决策树和神经网络使用最多
分类
（1）boosting算法： 个体学习器之间是串行的，代表算法是boosting系列算法
（2）bagging/RF系列算法：个体学习器之间是并行的

ref:
http://www.cnblogs.com/pinard/p/6131423.html

# bagging
Bagging 原理
（1）各个若学习器之间没有依赖关系
（2）随机采样：有放回的采样，gbdt的子采样是无放回的采样
	对于一个样本，某一次含m个训练集的随机采样中，每次被采集的概率是1/m。不被采样的概率为1-1/m。如果m次采样都没有采样到的概率是（1-1/m）^m ，
	当m-> ∞时 (1-1/m)^m = 1/e = 0.368。因此，在bagging中的每轮随机采样中，训练集中大约有36.8%的数据没有被采样到
（3）对于分类问题：得到多票数类别或者类别之一为最终输出； 
        对于回归问题：通常使用简单平均法，对T个若学习器得到的回归结果进行算数平均得到最终模型输出

RF（Random Forest）随机森林，是bagging的进化版
（1）使用CART决策树作为弱学习器
（2）对决策树的建立做了改进。对于普通的决策树，会在节点的所有n个样本特征中选择一个最优的特征来做决策树的左右子树划分； 
RF通过随机选择节点上的一部分特征nsub（< n），然后在nsub个特征中，选择一个最优的特征作为左右子树的划分，这样进一步增强模型的泛化能力
	（a）如果nsub=n，则是RF的CART决策树和普通的CART决策树没有区别
	（b）nsub越小，模型越健壮，模型方差会减少，但偏差会增大，此时对于训练集你和程度会变差。
	（c）在实际案例中，一般会通过交叉验证获取一个合适的nsub值

随机森林推广—可用于特征转换、异常点检测
（1）extra trees 和RF区别
	（a）RF采用随机采样，extra trees采用原始的训练集
	（b）RF会根据信息增益、基尼系数、均方差等原则选择一个最优的特征划分点，而extra trees采用随机选择一个特征

（2）Totall Random Trees Embedding
	将低维的数据映射到高维，从而高维的数据更好的运用分类回归模型

（3）Isolation Forest
	IForest时一种异常点检测的方法。

总结
（1）优点
	（a）训练可高度并行化
	（b）随机选择特征进行决策树节点划分，在特征维度很高时，仍然有高效的训练模型
	（c）由于采用了随机采样，训练出的模型方差小，泛化能力强
	（d）对部分特征缺失不敏感
（2）缺点
	（a）在噪声比较大的样本数据集上，RF模型容易陷入过拟合
	（b）取值划分比较多的特征容易对RF的决策产生更大的影响

ref
https://www.cnblogs.com/pinard/p/6156009.html

# boosting算法


https://www.zhihu.com/question/54626685/answer/140610056 (机器学习算法中GBDT与Adaboost的区别与联系是什么？)
