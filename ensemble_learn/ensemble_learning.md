[TOC]

# 模型融合的基本方法

通过组合集中机器学习模型来提升机器学习效果。通常的方法主要有降低方差(bagging), 减少偏差(boosting), 改进预测(stacking)。使组合的模型具有更强的泛化能力，在机器学习中，更强的泛化能力主要是降低Bias或者降低Variance。

Bias衡量了某种机器学习算法的平均估计结果所能逼近学习目标的程度，即Bias越低，模型的误差越低;

Variance度量了模型对于不同的数据集模型估计结果发生变动的程度, 可以认为是模型稳定性的一种度量方式，Variance越低说明模型越健壮，越稳定。

- bagging

bagging 全名为boostrap aggregration，boostrap是一种有放回的抽样。基本思想：学习算法训练多轮，每轮的训练集从初始的训练集中随机抽取n个训练样本，每个训练集中的样本可以出现一次或者多次，最终会得到一个预测序列 h1, h2, ..., hn。对于分类问题，采用投票方式; 对于回归问题，采用简单平均方式对新样本进行判别。基于bagging思想的模型主要有：random forest。

- boosting(<font color=red>知错就改</font>)

boosting将多个弱学习模型提升为强学习模型。基本思想：初始化时对每个训练样例赋予相等的权重1/n，然后训练t轮，每次训练后，对训练失败的样例赋予较大的权重，也就是需要模型在后续的学习中集中对比较难的训练样例进行学习。从而得到一个预测函数h1,h2,...,hn,其中预测好的函数权重较大，反而较小。对于分类问题，采用有权重的投票方式; 对于回归问题，采用加权平均方法对新样本进行判别。基于boosting思想的模型主要有：Adaboost, gbdt, xgboost

- bagging vs boosting

(1)训练抽样：bagging采用有放回抽样; boosting采用无放回抽样

(2)训练串、并行：bagging每一轮训练独立，易并行；boosting每一轮训练依赖上一轮结果，易串行

(3)bias and Variance：bagging采用反复抽样，训练强学习器，然后对强学习模型平均，并不能起到降低bias效果，而是使得模型稳定性加强，降低了Variance; 相反，boosting通过弱学习模型提升为强学习模型，降低bias



-------TODO
# ensemble learning(集成学习)
个体学习器选择
（1）个体学习器时同质的，比如决策树/神经网络
（2）个体学习器时异质的，比如支持向量机，逻辑回归，朴素贝叶斯
（3）通知个体学习器应用最广泛，其中CART决策树和神经网络使用最多
分类
（1）boosting算法： 个体学习器之间是串行的，代表算法是boosting系列算法
（2）bagging/RF系列算法：个体学习器之间是并行的

ref:
http://www.cnblogs.com/pinard/p/6131423.html

# bagging
Bagging 原理
（1）各个若学习器之间没有依赖关系
（2）随机采样：有放回的采样，gbdt的子采样是无放回的采样
	对于一个样本，某一次含m个训练集的随机采样中，每次被采集的概率是1/m。不被采样的概率为1-1/m。如果m次采样都没有采样到的概率是（1-1/m）^m ，
	当m-> ∞时 (1-1/m)^m = 1/e = 0.368。因此，在bagging中的每轮随机采样中，训练集中大约有36.8%的数据没有被采样到
（3）对于分类问题：得到多票数类别或者类别之一为最终输出； 
        对于回归问题：通常使用简单平均法，对T个若学习器得到的回归结果进行算数平均得到最终模型输出

RF（Random Forest）随机森林，是bagging的进化版
（1）使用CART决策树作为弱学习器
（2）对决策树的建立做了改进。对于普通的决策树，会在节点的所有n个样本特征中选择一个最优的特征来做决策树的左右子树划分； 
RF通过随机选择节点上的一部分特征nsub（< n），然后在nsub个特征中，选择一个最优的特征作为左右子树的划分，这样进一步增强模型的泛化能力
	（a）如果nsub=n，则是RF的CART决策树和普通的CART决策树没有区别
	（b）nsub越小，模型越健壮，模型方差会减少，但偏差会增大，此时对于训练集你和程度会变差。
	（c）在实际案例中，一般会通过交叉验证获取一个合适的nsub值

随机森林推广—可用于特征转换、异常点检测
（1）extra trees 和RF区别
	（a）RF采用随机采样，extra trees采用原始的训练集
	（b）RF会根据信息增益、基尼系数、均方差等原则选择一个最优的特征划分点，而extra trees采用随机选择一个特征

（2）Totall Random Trees Embedding
	将低维的数据映射到高维，从而高维的数据更好的运用分类回归模型

（3）Isolation Forest
	IForest时一种异常点检测的方法。

总结
（1）优点
	（a）训练可高度并行化
	（b）随机选择特征进行决策树节点划分，在特征维度很高时，仍然有高效的训练模型
	（c）由于采用了随机采样，训练出的模型方差小，泛化能力强
	（d）对部分特征缺失不敏感
（2）缺点
	（a）在噪声比较大的样本数据集上，RF模型容易陷入过拟合
	（b）取值划分比较多的特征容易对RF的决策产生更大的影响

ref
https://www.cnblogs.com/pinard/p/6156009.html

# boosting算法


https://www.zhihu.com/question/54626685/answer/140610056 (机器学习算法中GBDT与Adaboost的区别与联系是什么？)
