[TOC]

# 模型融合的基本方法

通过组合集中机器学习模型来提升机器学习效果。通常的方法主要有降低方差(bagging), 减少偏差(boosting), 改进预测(stacking)。使组合的模型具有更强的泛化能力，在机器学习中，更强的泛化能力主要是降低Bias或者降低Variance。

Bias衡量了某种机器学习算法的平均估计结果所能逼近学习目标的程度，即Bias越低，模型的误差越低;

Variance度量了模型对于不同的数据集模型估计结果发生变动的程度, 可以认为是模型稳定性的一种度量方式，Variance越低说明模型越健壮，越稳定。

- bagging

bagging 全名为boostrap aggregration，boostrap是一种有放回的抽样。基本思想：学习算法训练多轮，每轮的训练集从初始的训练集中随机抽取n个训练样本，每个训练集中的样本可以出现一次或者多次，最终会得到一个预测序列 h1, h2, ..., hn。对于分类问题，采用投票方式; 对于回归问题，采用简单平均方式对新样本进行判别。基于bagging思想的模型主要有：random forest。

- boosting

boosting将多个弱学习模型提升为强学习模型。基本思想：初始化时对每个训练样例赋予相等的权重1/n，然后训练t轮，每次训练后，对训练失败的样例赋予较大的权重，也就是需要模型在后续的学习中集中对比较难的训练样例进行学习。从而得到一个预测函数h1,h2,...,hn,其中预测好的函数权重较大，反而较小。对于分类问题，采用有权重的投票方式; 对于回归问题，采用加权平均方法对新样本进行判别。基于boosting思想的模型主要有：Adaboost, gbdt, xgboost

- bagging vs boosting

(1)训练抽样：bagging采用又放回抽样; boosting采用无放回抽样

(2)训练串、并行：bagging每一轮训练独立，易并行；boosting每一轮训练依赖上一轮结果，易串行

(3)bias and Variance：bagging采用反复抽样，训练强学习器，然后对强学习模型平均，并不能起到降低bias效果，而是使得模型稳定性加强，降低了Variance; 相反，boosting通过弱学习模型提升为强学习模型，降低bias


