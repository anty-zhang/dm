
[TOC]

# 余弦相似度和内积的区别

## 双塔模型最后一层用cos相似度，而faiss中内积来计算，该如何解释

- 余弦相似度和内积的计算公式

向量 a=(x1,x2,x3), b=(y1,y2,y3)

余弦相似度:

$$cos \theta= \frac {x_1*y_1 + x_2*y_2 + x_3*y_3} {\sqrt{x_1^2 + x_2^2 + x_3^2} * \sqrt{y_1^2 + y_2^2 + y_3^2}}$$

内积:

$$a . b = |a| * |b| * cos\theta$$

- 区别和物理意义

余弦相似度是归一化的内积。余弦相似度只考虑了角度差，内积综合了角度差和长度差。

例如: 如果向量的长度本身对相似度有影响，推荐使用内积。比如商品的打分1(不确定), ... , 5(很确定)，那么A=(1,1,1), B(4,4,4), C(5,5,5)三个商品，根据内积B与C更相似，但余弦相似度则无法区分A，B，C的相似性。

## word2vec训练得到的词向量，为什么用cos 而不是 内积 衡量相似度

- cos是归一化的内积，在训练过程中，词向量的长度会受到语料中词频的影响，而两个单词的词向量不应该受到词频的影响。

- 在训练过程中使用内积做相似度可以保留词频信息。

> 简单证明：假设非常理想的情况下，词向量初始化为0，词“test”在语料A中出现了1次，在语料B中出现了10次，并且每次出现的上下文完全相同。那么我们得到在A中进行一步训练得到的梯度是  ，相应地在B中累积十步训练得到的梯度是  。那么在语料A中“test”的词向量为  ，在语料B中为  .由于上下文完全一样，我们知道两个相差10倍的词向量其实代表一个意思，而这个系数10代表着出现频次。
>
> 总结：在频次信息重要的任务中，应当使用内积。而在频次不重要的任务中，使用余弦距离可以得到归一化的相似度表达，看起来更加工整且便于比较。

- 内积衡量向量相似度只是一种cosine相似度算法的一种简化，简化的前提是在同一个scale下。word2vector的embedding特征并不满足这前提，所以用的cosine相似度。

- cos还是内积相似度衡量讨论

[What meaning does the length of a Word2vec vector have?](https://stackoverflow.com/questions/36034454/what-meaning-does-the-length-of-a-word2vec-vector-have)

https://arxiv.org/pdf/1508.02297.pdf

https://arxiv.org/abs/2004.15003

# reference

[word2vec训练得到的词向量，为什么用cos 而不是 内积 衡量相似度?](https://www.zhihu.com/question/318407108)
