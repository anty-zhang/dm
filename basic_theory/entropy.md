
[TOC]

# 熵基本知识

## 熵

熵是什么？熵存在的意义是啥？为什么叫熵？这是3个非常现实的问题。答案非常明确：<font color='red'>在机器学习中熵是表征随机变量分布的混乱程度，分布越混乱，则熵越大，在物理学上表征物质状态的参量之一，也是体系混乱程度的度量；熵存在的意义是度量信息量的多少，人们常常说信息很多，或者信息较少，但却很难说清楚信息到底有多少，这时熵的意义就体现出来了</font>

物理生活中熵的理解：<font color=red>整个宇宙发展就是一个熵增的过程</font>，具体到细节就是气体扩散、热量传递、宇宙爆炸等等，如果不加干扰，几种气体肯定会混合到一起，任何比环境温度高的物体，都会把热量向低温环境散发，直到系统内温度平衡，所有的恒星终将熄灭，宇宙中不再有能量的流动，因而不可避免地走向无序。如果房间不去打扫，那么房间肯定越来越乱，这就是一个自然的熵增过程。如果不施加外力影响，事物永远向着更混乱的状态发展，故而人存在的意义就是通过自身努力改造自身、改造自然。借用一句话：过去五千年，人类文明的进步只是因为人类学会利用外部能量（牲畜、火种、水力等等），越来越多的能量注入，使得人类社会向着文明有序的方向发展即通过人类的努力使得熵值一直在下降。大家一起努力使得整个世界熵值下降的更多吧！！！ 

### 自信息/信息量

** 自信息表示某一事件发生时所带来的信息量的多少，当事件发生的概率越大，则自信息越小，或者可以这样理解：某一事件发生的概率非常小，但是实际上却发生了(观察结果)，则此时的自信息非常大；某一事件发生的概率非常大，并且实际上也发生了，则此时的自信息较小。**


首先是信息量。假设我们听到了两件事，分别如下： 
事件A：巴西队进入了2018世界杯决赛圈。 
事件B：中国队进入了2018世界杯决赛圈。 
仅凭直觉来说，显而易见事件B的信息量比事件A的信息量要大。究其原因，是因为事件A发生的概率很大，事件B发生的概率很小。所以当越不可能的事件发生了，我们获取到的信息量就越大。越可能发生的事件发生了，我们获取到的信息量就越小。那么信息量应该和事件发生的概率有关。


从通俗角度理解了自信息的含义和作用，但是如何度量它呢？我们现在要寻找一个函数，它要满足的条件是：

- 事件发生的概率越大，则自信息越小；
- 自信息不能是负值，最小是0；
- 自信息应该满足可加性，并且两个独立事件的自信息应该等于两个事件单独的自信息。

下面给出自信息的具体公式：
$I(p_i) = -log(p_i)$

![自信息](./img/entropy/zixinxi.png)

其中$p_i$表示随机变量的第i个事件发生的概率，自信息单位是bit,表征描述该信息需要多少位。可以看出，自信息的计算和随机变量本身数值没有关系，只和其概率有关，同时可以很容易发现上述定义满足自信息的3个条件。

## 信息熵/经验上/香浓熵

** 信息熵通常用来描述整个随机分布所带来的信息量平均值，更具统计特性。**信息熵也叫香农熵，在机器学习中，由于熵的计算是依据样本数据而来，故也叫经验熵。

<font color=red>熵代表了随机分布的混乱程度，这一特性是所有基于熵的机器学习算法的核心思想。 </font>

$I(p_i) = -log(p_i)$

$H(X) = E_{x \sim p}[I(x)] = -E_{x \sim p}[log p(x)] =-\sum_{i=1}^{n}p(x_i)logp(x_i)=-\int_{x} p(x)logp(x)d_x$

从公式可以看出，信息熵H(X)是各项自信息的累加值，由于每一项都是整正数，故而 ** 随机变量取值个数越多，状态数也就越多，累加次数就越多，信息熵就越大，混乱程度就越大，纯度越小。** 
越宽广的分布，熵就越大，在同样的定义域内，由于分布宽广性中 脉冲分布<高斯分布<均匀分布，故而熵的关系为脉冲分布信息熵<高斯分布信息熵<均匀分布信息熵。可以通过数学证明，当随机变量分布为均匀分布时即状态数最多时，熵最大。


## 相对熵/KL散度

相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异。即如果用P来描述目标问题，而不是用Q来描述目标问题，得到的信息增量。

在机器学习中，P往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。Q用来表示模型所预测的分布，比如[0.7,0.2,0.1] 
直观的理解就是如果用P来描述样本，那么就非常完美。而用Q来描述样本，虽然可以大致描述，但是不是那么的完美，信息量不足，需要额外的一些“信息增量”才能达到和P一样完美的描述。如果我们的Q通过反复训练，也能完美的描述样本，那么就不再需要额外的“信息增量”，Q等价于P。

KL散度的计算公式： 

$D_{KL}(p||q) = \sum_{i=1}^n p(x_i) log(\frac {p(x_i)} {q(x_i)})$

n: 事件的所有可能性
$D_{KL}$ 的值越小，表示q分布和p分布越接近

## 交叉熵

由KL散度公司推导如下

$D_{KL}(p||q) = \sum_{i=1}^{n}p(x_i)log(p(x_i)) - \sum_{i=1}^{n}p(x_i)log(q(x_i))=-H(p(x)) - \sum_{i=1}^{n}p(x_i)log(q(x_i))$

等式的前一部分恰巧就是p的熵，等式的后一部分，就是交叉熵：

$H(p, q) = - \sum_{i=1}^{n}p(x_i) log(q(x_i))$

在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即 $D_{KL}(y|| \hat y)$，由于KL散度中的前一部分−H(y)不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做loss，评估模型。


## reference

[关于交叉熵在loss函数中使用的理解](https://blog.csdn.net/tsyccnh/article/details/79163834)

[机器学习各种熵：从入门到全面掌握](https://mp.weixin.qq.com/s/LGyNq3fRlsRSatu1lpFnnw)

[为什么交叉熵（cross-entropy）可以用于计算代价](https://www.zhihu.com/question/65288314/answer/244557337)

[信息熵到底是什么](https://blog.csdn.net/saltriver/article/details/53056816)

[详解机器学习中的熵、条件熵、相对熵和交叉熵](http://www.cnblogs.com/kyrieng/p/8694705.html)

从香农熵到手推KL散度：纵览机器学习中的信息论 

熵：宇宙的终极规则 

https://zhuanlan.zhihu.com/p/26486223 (TODO)

https://www.zhihu.com/question/22104055 (TODO)