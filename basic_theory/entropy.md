
[TOC]

# 熵

熵是什么？熵存在的意义是啥？为什么叫熵？这是3个非常现实的问题。答案非常明确：<font color='red'>在机器学习中熵是表征随机变量分布的混乱程度，分布越混乱，则熵越大，在物理学上表征物质状态的参量之一，也是体系混乱程度的度量；熵存在的意义是度量信息量的多少，人们常常说信息很多，或者信息较少，但却很难说清楚信息到底有多少，这时熵的意义就体现出来了</font>

物理生活中熵的理解：<font color=red>整个宇宙发展就是一个熵增的过程</font>，具体到细节就是气体扩散、热量传递、宇宙爆炸等等，如果不加干扰，几种气体肯定会混合到一起，任何比环境温度高的物体，都会把热量向低温环境散发，直到系统内温度平衡，所有的恒星终将熄灭，宇宙中不再有能量的流动，因而不可避免地走向无序。如果房间不去打扫，那么房间肯定越来越乱，这就是一个自然的熵增过程。如果不施加外力影响，事物永远向着更混乱的状态发展，故而人存在的意义就是通过自身努力改造自身、改造自然。借用一句话：过去五千年，人类文明的进步只是因为人类学会利用外部能量（牲畜、火种、水力等等），越来越多的能量注入，使得人类社会向着文明有序的方向发展即通过人类的努力使得熵值一直在下降。大家一起努力使得整个世界熵值下降的更多吧！！！ 

## 自信息/信息量

** 自信息表示某一事件发生时所带来的信息量的多少，当事件发生的概率越大，则自信息越小，或者可以这样理解：某一事件发生的概率非常小，但是实际上却发生了(观察结果)，则此时的自信息非常大；某一事件发生的概率非常大，并且实际上也发生了，则此时的自信息较小。**


首先是信息量。假设我们听到了两件事，分别如下： 
事件A：巴西队进入了2018世界杯决赛圈。 
事件B：中国队进入了2018世界杯决赛圈。 
仅凭直觉来说，显而易见事件B的信息量比事件A的信息量要大。究其原因，是因为事件A发生的概率很大，事件B发生的概率很小。所以当越不可能的事件发生了，我们获取到的信息量就越大。越可能发生的事件发生了，我们获取到的信息量就越小。那么信息量应该和事件发生的概率有关。


从通俗角度理解了自信息的含义和作用，但是如何度量它呢？我们现在要寻找一个函数，它要满足的条件是：

- 事件发生的概率越大，则自信息越小；
- 自信息不能是负值，最小是0；
- 自信息应该满足可加性，并且两个独立事件的自信息应该等于两个事件单独的自信息。

下面给出自信息的具体公式：
$I(p_i) = -log(p_i)$

![自信息](./img/entropy/zixinxi.png)

其中$p_i$表示随机变量的第i个事件发生的概率，自信息单位是bit,表征描述该信息需要多少位。可以看出，自信息的计算和随机变量本身数值没有关系，只和其概率有关，同时可以很容易发现上述定义满足自信息的3个条件。

本质上，自信息建立了 一个随机事件发生的概率 和 信息量之间的关系

# 信息熵/经验熵/香浓熵

** 信息熵通常用来描述整个随机分布所带来的信息量平均值，更具统计特性。**信息熵也叫香农熵，在机器学习中，由于熵的计算是依据样本数据而来，故也叫经验熵。

<font color=red>熵代表了随机分布的混乱程度，这一特性是所有基于熵的机器学习算法的核心思想。 </font>

$I(p_i) = -log(p_i)$

$H(X) = E_{x \sim p}[I(x)] = -E_{x \sim p}[log p(x)] =-\sum_{i=1}^{n}p(x_i)logp(x_i)=-\int_{x} p(x)logp(x)d_x$

从公式可以看出，信息熵H(X)是各项自信息的累加值，由于每一项都是整正数，故而 ** 随机变量取值个数越多，状态数也就越多，累加次数就越多，信息熵就越大，混乱程度就越大，纯度越小。** 
越宽广的分布，熵就越大，在同样的定义域内，由于分布宽广性中 脉冲分布<高斯分布<均匀分布，故而熵的关系为脉冲分布信息熵<高斯分布信息熵<均匀分布信息熵。可以通过数学证明，当随机变量分布为均匀分布时即状态数最多时，熵最大。

# 联合熵

$$H(X,Y) = -\sum_{i=1}^n \sum_{j=1}^m p(x_i, y_i) log(p(x_i, y_i))$$

- 熵只依赖于随机变量的分布,与随机变量取值无关;

- 定义0log0=0(因为可能出现某个取值概率为0的情况);

- 熵越大,随机变量的不确定性就越大,分布越混乱，随机变量状态数越多

# 条件熵

定义: 在给定X的条件下，Y的条件概率分布的熵 对 X 的数学期望。

$H(Y|X) = E_{x-p} H(Y| X = x) = \sum_{i=1}^n p(x) H(Y|X=x)$

$= - \sum_{i=1}^n p(x) \sum_{j=1}^m p(y|x) log(p(y|x))$

$= - \sum_{i=1}^n \sum_{j=1}^m p(x) p(y|x) log(p(y|x))$

$= - \sum_{i=1}^n \sum_{j=1}^m p(x, y) log(p(y|x))$

$= - (\sum_{i=1}^n \sum_{j=1}^m p(x, y) log(p(x, y)) - \sum_{i=1}^n \sum_{j=1}^m p(x, y) log(p(x)))$

$= - (\sum_{i=1}^n \sum_{j=1}^m p(x, y) log(p(x, y)) - \sum_{i=1}^n log(p(x)) \sum_{j=1}^m p(x,y))$

$= - (\sum_{i=1}^n \sum_{j=1}^m p(x, y) log(p(x, y)) - \sum_{i=1}^n p(x)log(p(x)))$

$= H(X, Y) - H(X)$

同理 $H(X|Y) = H(X, Y) - H(Y)$

[通俗理解条件熵](https://zhuanlan.zhihu.com/p/26551798)

# 相对熵/KL散度

相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 <font color=red> KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异。即如果用P来描述目标问题，而不是用Q来描述目标问题，得到的信息增量。</font>

KL散度，有时候也叫KL距离，一般被用于计算两个分布之间的不同。看名字似乎跟计算两个点之间的距离也很像，但实则不然，因为 <font color=red>KL散度不具备有对称性</font>。在距离上的对称性指的是A到B的距离等于B到A的距离。

在机器学习中，P往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。Q用来表示模型所预测的分布，比如[0.7,0.2,0.1]
直观的理解就是如果用P来描述样本，那么就非常完美。而用Q来描述样本，虽然可以大致描述，但是不是那么的完美，信息量不足，需要额外的一些“信息增量”才能达到和P一样完美的描述。如果我们的Q通过反复训练，也能完美的描述样本，那么就不再需要额外的“信息增量”，Q等价于P。

举个不恰当的例子，事件A：张三今天买了2个土鸡蛋，事件B：李四今天买了6个土鸡蛋。我们定义随机变量x：买土鸡蛋，那么事件A和B的区别是什么？有人可能说，那就是李四多买了4个土鸡蛋？这个答案只能得50分，因为忘记了&#34;坐标系&#34;的问题。换句话说，对于张三来说，李四多买了4个土鸡蛋。对于李四来说，张三少买了4个土鸡蛋。<b>选取的参照物不同，那么得到的结果也不同</b>。更严谨的说，应该是说我们对于张三和李四买土鸡蛋的期望不同，可能张三天天买2个土鸡蛋，而李四可能因为孩子满月昨天才买了6个土鸡蛋，而平时从来不买。

KL散度的计算公式： 

离散事件：$D_{KL}(p||q) = \sum_{i=1}^n p(x_i) log(\frac {p(x_i)} {q(x_i)})$

n: 事件的所有可能性
$D_{KL}$ 的值越小，表示q分布和p分布越接近

连续事件：$D_{KL}(p||q)= \int a(x) log(\frac {a(x)} {b(x)})$

- p(x) 和q(x)概率相等时，两个事件分布完全相同，KL散度为0

- KL散度由A自己的熵 和 B在A上的期望共同完决定的。当使用KL散度来衡量两个事件(连续或离散)，上面的公式意义就是求A和B之前的对数差 在A上的期望。

- KL 散度是非负的

- KL散度不具有对称性。通过公式可以看出，KL散度是衡量两个分布的不相似性，不相似性越大，则值越大，当完全相同时，取值为0。

# 交叉熵

交叉熵主要度量两个概率分布间的差异性信息，p对q的交叉熵表示q分布的自信息对p分布的期望，公式为:

$H(p,q) = E_{x~p} [-log(q(x))] = -\sum_{i=1}^n p(x) log(q(x))$

由KL散度公司推导如下

$D_{KL}(p||q) = \sum_{i=1}^{n}p(x_i)log(p(x_i)) - \sum_{i=1}^{n}p(x_i)log(q(x_i))=-H(p(x)) - \sum_{i=1}^{n}p(x_i)log(q(x_i))$

等式的前一部分恰巧就是p的熵，等式的后一部分，就是交叉熵：

$H(p, q) = - \sum_{i=1}^{n}p(x_i) log(q(x_i))$

在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即 $D_{KL}(y|| \hat y)$，由于KL散度中的前一部分−H(y)不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做loss，评估模型。

- 性质

> 交叉熵也不具有对称性: 即 $H(p, q) \neq H(q, p)$

# KL散度、交叉熵比较

> 问题：为什么会有交叉熵和KL散度两种算法？为什么可以求分布的不同？什么时候可以等价使用？

- 信息论的解释

> 熵的意义: 对事件A中的随机变量进行编码所需的最小字节数

> KL散度的意义: 如果用Q编码来表示P，“额外所需要编码的长度”

> 交叉熵的意义: 当使用Q作为密码本来表示P时所需要的“平均编码长度”

- 对比

> KL散度和交叉熵的不同处：交叉熵中不包含 “熵”的部分； 相同处: 都是非负的，都不具有对称性
>
> 等价条件：当P固定不变时，那么优化最小散度 $D_{KL}(P||Q)$ 等价于优化最小交叉熵 $D_{KL}(P||Q) = H(P, Q)$

- 相对熵大量应用在生成模型中，例如GAN、EM、贝叶斯学习和变分推导中。

> 需要明确的知道生成的分布和真实分布的差距，最好的KL散度值应该是0。

- 交叉熵主要应用在判别模型中

> 仅仅只需要评估损失函数的下降值即可，交叉熵可以满足要求，其计算量比KL散度小。

## 互信息

互信息可以评价两个分布之间的距离，这主要归因于其对称性，假设互信息不具备对称性，那么就不能作为距离度量，例如相对熵，由于不满足对称性，故通常说相对熵是评价分布的相似程度，而不会说距离。

互信息的定义为：一个随机变量由于已知另一个随机变量而减少的不确定性，或者说从贝叶斯角度考虑，由于新的观测数据y到来而导致x分布的不确定性下降程度。

- 公式


- 性质

> 当且仅当X和Y为独立随机变量时，互信息值为0. p(x,y) = p(x)p(y), 所以 $log(\frac {p(x,y)} {p(x)p(y)}) = log(1) =0$

> 互信息是对偶的: I(X; Y) = I(Y; X)

> 互信息是非负的

> 连续性的特征也需要离散化处理

> 弥补相对熵的缺陷。如果说相对熵不能作为距离度量，是因为其非对称性，那么互信息的出现正好弥补了该缺陷，使得我们可以计算任意两个随机变量之间的距离，或者说两个随机变量分布之间的相关性、独立性。

[互信息（Mutual Information）](https://www.cnblogs.com/gatherstars/p/6004075.html)

[互信息公式及概述](https://www.omegaxyz.com/2018/08/02/mi/)

# 信息增益

信息增益是决策树ID3算法在进行特征切割时使用的划分准则，其物理意义和互信息完全相同，并且公式也是完全相同。其公式如下:

$g(D, A) = H(D) - H(D|A)$

其中D表示数据集，A表示特征，信息增益表示得到A的信息而使得类X的不确定度下降的程度，在ID3中，需要选择一个A使得信息增益最大，这样可以使得分类系统进行快速决策。

需要注意的是：在数值上，信息增益和互信息完全相同，但意义不一样，需要区分，当我们说互信息时候，两个随机变量的地位是相同的，可以认为是纯数学工具，不考虑物理意义，当我们说信息增益时候，是把一个变量看成是减少另一个变量不确定度的手段。

缺点：信息增益偏向取值较多的特征

原因：当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分之后的熵更低，由于划分前的熵是一定的，因此信息增益更大，因此信息增益比较 偏向取值较多的特征。

# 信息增益率

信息增益率是决策树C4.5算法引入的划分特征准则，其主要是克服信息增益存在的在某种特征上分类特征细，但实际上无意义取值时候导致的决策树划分特征失误的问题。例如假设有一列特征是身份证ID，每个人的都不一样，其信息增益肯定是最大的，但是对于一个情感分类系统来说，这个特征是没有意义的，此时如果采用ID3算法就会出现失误，而C4.5正好克服了该问题。其公式如下:

$g_r(D, A) = g(D, A)/H(A)$

缺点：信息增益比偏向取值较少的特征

原因：当特征取值较少时HA(D)的值较小，因此其倒数较大，因而信息增益比较大。因而偏向取值较少的特征。

# 基尼系数

基尼系数是决策树CART算法引入的划分特征准则，其提出的目的不是为了克服上面算法存在的问题，而主要考虑的是计算快速性、高效性，这种性质使得CART二叉树的生成非常高效。其公式如下：

$gini(p) = \sum_{i=1}^n p_i ( 1-p_i) = 1 - \sum_{i=1}^n p_i^2 = 1 - \sum_{i=1}^n (\frac {|C_k|} {D})^2$

可以看出，基尼系数越小，表示选择该特征后熵下降最快，对分类模型效果更好，其和信息增益和信息增益率的选择指标是相反的。基尼系数主要是度量数据划分对训练数据集D的不纯度大小，基尼系数越小，表明样本的纯度越高。

这里还存在一个问题，这个公式显得非常突兀，感觉突然就出来了，没有那种从前人算法中改进而来的感觉？其实为啥说基尼系数计算速度快呢，因为基尼系数实际上是信息熵的一阶进似，作用等价于信息熵，只不过是简化版本。根据泰勒级数公式，将 $f(x) = -ln(x)$ 在x=1处展开，忽略高阶无穷小，其可以等价为 $f(x) = 1-x$，所以可以很容易得到上述定义。

[决策树--CART树详解(举例)](https://www.cnblogs.com/wqbin/p/11689709.html)

# 理解相关问题

## 机器学习如何学习

机器学习的过程就是希望在训练数据上<b>模型学到的分布 P(model) 和 真实数据的分布 P(real) </b> 越接近越好，那么我们已经介绍过了....怎么最小化两个分布之间的不同呢？<b>用默认的方法，使其KL散度最小</b>

但我们没有真实数据的分布，那么只能退而求其次，希望<b>模型学到的分布和 训练数据的分布 P(training) </b> 尽量相同，<b>也就是把训练数据当做模型和真实数据之间的代理人</b>。

假设训练数据是从总体中独立同步分布采样(Independent and identically distributed sampled)而来，那么我们可以利用最小化训练数据的经验误差来降低模型的泛化误差。简单说：

- 最终目的是希望学到的模型的分布和真实分布一致：$P(model) \approx P(real)$

- 但真实分布是不可知的，我们只好假设 训练数据 是从真实数据中独立同分布采样而来：$P(training) \approx P(real)$

- 退而求其次，我们希望学到的模型分布至少和训练数据的分布一致: $P(model) \approx P(training)$ 

由此非常理想化的看法是如果<b>模型(左)</b>能够学到<b>训练数据(中)</b>的分布，那么应该近似的学到了<b>真实数据(右)</b>的分布：$P(model) \approx P(training) \approx P(real)$


## 为什么交叉熵可以用作代价？

接着上一点说，最小化模型分布 P(model) 与 训练数据上的分布 P(training) 的差异 等价于 最小化这两个分布间的KL散度，也就是最小化 $D_{KL}(P(training)||P(model))$。比照第四部分的公式：

- 此处的A就是数据的真实分布： P(training)

- 此处的B就是模型从训练数据上学到的分布： P(model)

巧的是，<b>训练数据的分布A是给定的。那么根据我们在第四部分说的，因为A固定不变，那么求 $D_{KL}(A||B)$ 等价于求 H(A, B) ，也就是A与B的交叉熵。 得证，交叉熵可以用于计算“学习模型的分布”与“训练数据分布”之间的不同。当交叉熵最低时(等于训练数据分布的熵)，我们学到了“最好的模型”。</b>

<b>但是，完美的学到了训练数据分布往往意味着过拟合，因为训练数据不等于真实数据，我们只是假设它们是相似的，而一般还要假设存在一个高斯分布的误差，是模型的泛化误差下线。</b>

# reference

[关于交叉熵在loss函数中使用的理解](https://blog.csdn.net/tsyccnh/article/details/79163834)

[机器学习各种熵：从入门到全面掌握](https://mp.weixin.qq.com/s/LGyNq3fRlsRSatu1lpFnnw)

[为什么交叉熵（cross-entropy）可以用于计算代价](https://www.zhihu.com/question/65288314/answer/244557337)

[信息熵到底是什么](https://blog.csdn.net/saltriver/article/details/53056816)

[详解机器学习中的熵、条件熵、相对熵和交叉熵](http://www.cnblogs.com/kyrieng/p/8694705.html)

从香农熵到手推KL散度：纵览机器学习中的信息论

熵：宇宙的终极规则

[线性相关系数、卡方检验、互信息](https://blog.csdn.net/gdanskamir/article/details/54913233)

[信息增益到底怎么理解呢？](https://www.zhihu.com/question/22104055)

[详解机器学习中的熵、条件熵、相对熵和交叉熵](https://www.cnblogs.com/kyrieng/p/8694705.html)
